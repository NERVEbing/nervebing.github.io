<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Problem on Lv&#39;s Playground</title>
    <link>https://lvlv.fun/tags/problem/</link>
    <description>Recent content in Problem on Lv&#39;s Playground</description>
    <generator>Hugo -- 0.146.5</generator>
    <language>en</language>
    <lastBuildDate>Mon, 22 Jul 2019 12:40:20 +0800</lastBuildDate>
    <atom:link href="https://lvlv.fun/tags/problem/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>gRPC Connection reset by peer 问题</title>
      <link>https://lvlv.fun/posts/2019-07-22/</link>
      <pubDate>Mon, 22 Jul 2019 12:40:20 +0800</pubDate>
      <guid>https://lvlv.fun/posts/2019-07-22/</guid>
      <description>&lt;h2 id=&#34;前提&#34;&gt;前提&lt;/h2&gt;
&lt;p&gt;因产品需求，需用 PHP（v7.0.12）调用 k8s 集群中 gRPC（golang@1.12.0）服务。&lt;/p&gt;
&lt;h2 id=&#34;问题&#34;&gt;问题&lt;/h2&gt;
&lt;p&gt;经过 dev 环境测试，当 php-fpm 启动后第一次调用或者距离上次调用时间 20 分钟后左右，再次请求 gRPC 微服务接口，就会返回
Connection reset by peer 错误，说明 gRPC 服务端或者客户端主动关闭连接了。继续发起请求到服务端，又恢复正常。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;gRPC-q1-image1&#34; loading=&#34;lazy&#34; src=&#34;https://i.loli.net/2019/07/22/5d353d7749a3955573.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;思考&#34;&gt;思考&lt;/h2&gt;
&lt;p&gt;经查阅相关资料，发现问题可能出现在 k8s 集群的 kube-proxy 模式上。当前 k8s 环境 (dev) 下的 kube-proxy 为 ipvs
模式，服务端与客户端之间通信如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;gRPC-q1-image2&#34; loading=&#34;lazy&#34; src=&#34;https://i.loli.net/2019/07/22/5d353d77d744e46363.jpg&#34;&gt;&lt;/p&gt;
&lt;p&gt;把上图 client 看成是 apsopen-inside 服务 pod，Backend pod(1~3) 看成 gRPC 服务，可以看出它们之间的交互路径：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                      ---&amp;gt; gRPC server pod1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;gRPC-client ---&amp;gt; ipvs ---&amp;gt; gRPC server pod3
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                      ---&amp;gt; gRPC server pod3
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;我们知道gRPC是基于HTTP/2协议的，gRPC的client和server在交互时会建立多条连接，为了性能，这些连接都是长连接并且是一直保活的。这段环境中不管是客户端服务还是
gRPC 服务都被调度到各个相同配置信息的 Kubernetes 节点上，这些 k8s 节点的 keep-alive 是一致的，如果出现连接主动关闭的问题，因为从
client 到 server 经历了一层 ipvs，所以最大的可能就是 ipvs 出将连接主动断开，而 client 端还不知情。
搜索 ipvs timeout 关键字找到了下面相关的链接：&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
